1. reddit data scrapper -> csv file -> use airflow to upload to our s3 bucket (call it landing bucket)
	-airflow should run every hourly -> scrap the data via python operator
2. use aws lambda to do some simple transformations to the data
	-really is just cleanup here i think (remove useless columns, same letter case, strip punctuation, filter stop words, part of speech tagging? i really only 	want nouns when you think about it)
	-since the cleanup is relatively simple, write a simple pyspark job for lambda? maybe not even spark cause its really not that hard
	-not hard, but remember, its suppose to handle a lot of data and spark does that in spades
3. output cleaned file to another s3 bucket (call this target bucket)
4. not entirely sure about this part; do i use glue, athena or quicksight to pull data? probably quicksight and use quicksight to pull data
	-don't think glue because my table structure is already pretty consistent


to-do list
1. set up S3 buckets (landing, target, maybe need an archive?)
2. write a DAG for the python scrapping operation -> git commit it -> potentially test it
3. write pyspark job for lambda/emr usage (if lambda -> no DAG, if emr -> DAG) -> load it wherever thats needed
4. read up on quicksight -> lets make a very simple analysis of the data
